\documentclass[landscape,a0paper,fontscale=0.292]{baposter}


%\usepackage{algpseudocode,algorithm}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{times}
\usepackage{calc}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{multirow}
\usepackage{booktabs}

\usepackage{graphicx}
\usepackage{multicol}
\usepackage[T1]{fontenc}
\usepackage{ae}

\graphicspath{{images/}}

 % Multicol Settings
 \setlength{\columnsep}{0.7em}
 \setlength{\columnseprule}{0mm}

 % Save space in lists. Use this after the opening of the list
 \newcommand{\compresslist}{%
 \setlength{\itemsep}{1pt}%
 \setlength{\parskip}{0pt}%
 \setlength{\parsep}{0pt}%
 }

 % Formating
 \newcommand*{\norm}[1]{\mathopen\| #1 \mathclose\|}% use instead of $\|x\|$
 \newcommand*{\abs}[1]{\mathopen| #1 \mathclose|}% use instead of $\|x\|$
 \newcommand*{\normLR}[1]{\left\| #1 \right\|}% use instead of $\|x\|$
 \DeclareMathOperator*{\argmax}{argmax}
 \DeclareMathOperator*{\diag}{diag}
 \DeclareMathOperator*{\argmin}{argmin}
 \DeclareMathOperator*{\vectorize}{vec}
 \DeclareMathOperator*{\reshape}{reshape}
 \font\dsfnt=dsrom12
 \DeclareSymbolFont{nark}{U}{dsrom}{m}{n}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Begin of Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Here starts the poster
%%---------------------------------------------------------------------------
%% Format it to your taste with the options
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{poster}{
    % Show grid to help with alignment
    grid=false,
    % Column spacing
    colspacing=0.7em,
    % Color style
    headerColorOne=cyan!20!white!90!black,
    borderColor=cyan!30!white!90!black,
    % Format of textbox
    textborder=faded,
    % Format of text header
    headerborder=open,
    headershape=roundedright,
    headershade=plain,
    background=none,
    bgColorOne=cyan!10!white,
    headerheight=0.12\textheight
}
    % Eye Catcher
    {
         \includegraphics[height=0.1\textheight]{ucla_campus_seal}
    }
    % Title
    {\sc\Huge A Brief Survey of Deep Q-Learning Methods}
    % Authors
    {Ryan Chau, My-Quan Hong, Nathan Kang, and Chris Munoz\\[0.5em]
    {\texttt{chau\_ryan@yahoo.com, myquan@yahoo.com, nkang@seis.ucla.edu,
    cmunozcortes@ucla.edu}}}
    % University logo
    {
     \begin{tabular}{r}
       \includegraphics[height=0.05\textheight]{logo_UCLA_blue}
     \end{tabular}
    }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Now define the boxes that make up the poster
%---------------------------------------------------------------------------
% Each box has a name and can be placed absolutely or relatively.
% The only inconvenience is that you can only specify a relative position 
% towards an already declared box. So if you have a box attached to the 
% bottom, one to the top and a third one which should be inbetween, you 
% have to specify the top and bottom boxes before you specify the middle 
% box.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Issues with Deep Q Networks}{name=dqn, column=0, row=0, span=2}{
    Despite being one of the most popular reinforcement algorithms,
    \textbf{Q-learning} performs poorly in some stochastic environments. The
    reason is that it produces overoptimistic estimates of the action values due
    to the max operator that is used for action selection and evaluation with
    the same network. \textbf{Double Q-learning} addresses this issue by
    decoupling action selection from action evaluation using two networks: an
    online network to select the greedy action, and a target network for
    evaluation.  The issue of overestimation of the action values is also true
    for \textbf{Deep Q Networks}, which combine Q-learning with a flexible deep
    neural network to approximate the action-value function. With a simple, yet
    effective, modification, the idea of double Q-learning applied to DQN gave
    rise to a new algorithm known as \textbf{Double DQN} (or DDQN).
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Deep Q-Learning}{name=ddqn, column=0, below=dqn}{
    \begin{algorithm}[H]
        \caption{DQN with experience replay}
        \SetAlgoLined
        \DontPrintSemicolon
          Initialize replay memory $\mathcal{D}$ to capacity $N$\;
          Initialize $Q$ with random weights $\theta$\;
          Initialize target action-value function $\smash{\hat{Q}}$ with weights
          $\theta^- = \theta$\;
          \For{$m \gets 1, \dots, M$} {
              Observe initial frame $x_1$ and preprocess frame to get $s_1$\;
              \For{$t \gets 1, \dots, T$} {
                  With probability $\varepsilon$ select a random action $a_t$;
                  otherwise select $a_t = \argmax_a Q(s_t, a; \theta)$\;
                  Execute action $a_t$, observe $r_t$, and image $x_{t+1}$\;
                  Preprocess $s_t, x_{t+1}$ to get $s_{t+1}$\;
                  Store transition $(s_t, a_t, r_t, s_{t+1})$ in $\mathcal{D}$\;
                  Sample minibatch of $N$ transitions $(s_j, a_j, r_j, s_{j+1})$
                  from $\mathcal{D}$\;
                  Set $y_j = r_j$ if episode ends at step $j+1$; otherwise set
                  $y_j = r_j + \gamma \max_{a'} \smash{\hat{Q}(s_{j+1}, a';
                  \theta^-)}$\;
                  Perform SGD step on $J(\theta) = \tfrac{1}{N} \sum_{j=1}^N
                  (y_j - Q(s_j, a_j; \theta))^2$ w.r.t to $\theta$\;
                  Every $c$ steps reset $\smash{\hat{Q}} = Q$\;
              }
          }
    \end{algorithm}
}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Experimental Results}{name=results,column=2,row=0,span=2}{
    \begin{multicols}{2}
        We trained and evaluated \textbf{DQN}, \textbf{Double DQN}, and
        \textbf{Dueling DQN} models on four Atari 2600 games from the Atari
        Learning Environment (ALE): breakout, road runner, boxing, and video
        pinball. The models were trained for 200 epochs (roughly 5M training
        steps) and evaluated for 100 episodes to get a mean score for each game.
        The mean scores were normalized the same way as in \cite{van2016deep} to
        show a comparison to human performance.
    \end{multicols}
}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{References}{name=references,column=0,above=bottom}{
    \smaller
  
    \bibliographystyle{ieee}
    \renewcommand{\section}[2]{\vskip 0.05em}
    \begin{thebibliography}{1}\itemsep=-0.01em
        \setlength{\baselineskip}{0.4em}

        \bibitem{van2016deep}
        H.~van~Hasselt, A.~Guez, D.~Silver.
        \newblock Deep Reinforcement Learning with Double Q-learning
        \newblock In {\em Proceedings of the AAAI Conference on Artificial
        Intelligence}, 2016.

        \bibitem{wang2016dueling}
        Z.~Wang, T.~Schaul, M.~Hessel, H.~van~Hasselt, M.~Lanctot, and
        N.~Freitas.
        \newblock Dueling Network Architectures for Deep Reinforcement Learning
        \newblock In {\em International Conference on Machine Learning}, 2016.

  \end{thebibliography}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Mean Scores in Atari 2600 Games}{name=scores, column=2, span=2,
    below=results, above=bottom}{
    %\begin{multicols}{2}
    %\end{multicols}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Dueling DQN}{name=dueling, column=1, span=1, above=bottom}{

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Double DQN}{name=algorithm, column=1, above=dueling,
    below=dqn}{
    Double DQN introduces a new learning target where action selection is done
    according to the online network with parameters $\theta$, and action
    evaluation is done by the target network with parameters $\theta^-$.
    \begin{align}
        \boxed{y_i^{\text{DDQN}} = r + \gamma \hat{Q} \left(s',
        \argmax_{a'} \hat{Q} \left(s', a'; \theta^-\right)\right)}
    \end{align}
}
\end{poster}%
%
\end{document}
