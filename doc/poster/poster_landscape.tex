\documentclass[landscape,a0paper,fontscale=0.32]{baposter}


%\usepackage{algpseudocode,algorithm}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{times}
\usepackage{calc}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{multirow}
\usepackage{booktabs}

\usepackage{graphicx}
\usepackage{multicol}
\usepackage[T1]{fontenc}
\usepackage{ae}

\graphicspath{{images/}}

 % Multicol Settings
 \setlength{\columnsep}{0.7em}
 \setlength{\columnseprule}{0mm}

 % Save space in lists. Use this after the opening of the list
 \newcommand{\compresslist}{%
 \setlength{\itemsep}{1pt}%
 \setlength{\parskip}{0pt}%
 \setlength{\parsep}{0pt}%
 }

 % Formating
 \newcommand*{\norm}[1]{\mathopen\| #1 \mathclose\|}% use instead of $\|x\|$
 \newcommand*{\abs}[1]{\mathopen| #1 \mathclose|}% use instead of $\|x\|$
 \newcommand*{\normLR}[1]{\left\| #1 \right\|}% use instead of $\|x\|$
 \DeclareMathOperator*{\argmax}{argmax}
 \DeclareMathOperator*{\diag}{diag}
 \DeclareMathOperator*{\argmin}{argmin}
 \DeclareMathOperator*{\vectorize}{vec}
 \DeclareMathOperator*{\reshape}{reshape}
 \font\dsfnt=dsrom12
 \DeclareSymbolFont{nark}{U}{dsrom}{m}{n}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Begin of Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Here starts the poster
%%---------------------------------------------------------------------------
%% Format it to your taste with the options
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{poster}{
    % Show grid to help with alignment
    grid=false,
    % Column spacing
    colspacing=0.7em,
    % Color style
    headerColorOne=cyan!20!white!90!black,
    borderColor=cyan!30!white!90!black,
    % Format of textbox
    textborder=faded,
    % Format of text header
    headerborder=open,
    headershape=roundedright,
    headershade=plain,
    background=none,
    bgColorOne=cyan!10!white,
    headerheight=0.12\textheight
}
    % Eye Catcher
    {
         \includegraphics[height=0.1\textheight]{ucla_campus_seal}
    }
    % Title
    {\sc\Huge A Brief Survey of Deep Q-Learning Methods}
    % Authors
    {Ryan Chau, My-Quan Hong, Nathan Kang, and Chris Munoz\\[0.5em]
    {\texttt{chau\_ryan@yahoo.com, myquan@yahoo.com, nkang@seis.ucla.edu,
    cmunozcortes@ucla.edu}}}
    % University logo
    {
     \begin{tabular}{r}
       \includegraphics[height=0.05\textheight]{logo_UCLA_blue}
     \end{tabular}
    }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Now define the boxes that make up the poster
%---------------------------------------------------------------------------
% Each box has a name and can be placed absolutely or relatively.
% The only inconvenience is that you can only specify a relative position 
% towards an already declared box. So if you have a box attached to the 
% bottom, one to the top and a third one which should be inbetween, you 
% have to specify the top and bottom boxes before you specify the middle 
% box.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Value-based Deep Reinforcement Learning Algorithms}{name=dqn,
    column=0, row=0, span=2}{
    \textbf{In this project we examine three popular value-based reinforcement
    learning algorithms: Deep Q-Network (DQN), Double DQN (DDQN), and Dueling
    DQN}. All three architectures are able to learn policies directly from
    high-dimensional inputs, e.g. preprocessed pixels from video games, using
    end-to-end reinforcement learning, and achieve performance comparable to
    expert human level on Atari 2600 games.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Deep Q-Learning}{name=ddqn, column=0, below=dqn}{
    \begin{algorithm}[H]
        \caption{DQN with experience replay}
        \SetAlgoLined
        \DontPrintSemicolon
          Initialize replay memory $\mathcal{D}$ to capacity $N$\;
          Initialize $Q$ with random weights $\theta$\;
          Initialize target action-value function $\smash{\hat{Q}}$ with weights
          $\theta^- = \theta$\;
          \For{$m \gets 1, \dots, M$} {
              Observe initial frame $x_1$ and preprocess frame to get $s_1$\;
              \For{$t \gets 1, \dots, T$} {
                  With probability $\varepsilon$ select a random action $a_t$;
                  otherwise select $a_t = \argmax_a Q(s_t, a; \theta)$\;
                  Execute action $a_t$, observe $r_t$, and image $x_{t+1}$\;
                  Preprocess $s_t, x_{t+1}$ to get $s_{t+1}$\;
                  Store transition $(s_t, a_t, r_t, s_{t+1})$ in $\mathcal{D}$\;
                  Sample minibatch of $N$ transitions $(s_j, a_j, r_j, s_{j+1})$
                  from $\mathcal{D}$\;
                  Set $y_j = r_j$ if episode ends at step $j+1$; otherwise set
                  $y_j = r_j + \gamma \max_{a'} \smash{\hat{Q}(s_{j+1}, a';
                  \theta^-)}$\;
                  Perform SGD step on $J(\theta) = \tfrac{1}{N} \sum_{j=1}^N
                  (y_j - Q(s_j, a_j; \theta))^2$ w.r.t to $\theta$\;
                  Every $c$ steps reset $\smash{\hat{Q}} = Q$\;
              }
          }
    \end{algorithm}
    Note that the learning target is,
    \begin{align*}
        \boxed{y_i^{\text{DQN}} = r + \gamma \max_{a'}
            \hat{Q}(s', a'; \theta^-)}
    \end{align*}
    where $\theta^-$ are the parameters of the \emph{target} network.
}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Experimental Results}{name=results,column=2,row=0,span=2}{
    \begin{multicols}{2}
        We trained and evaluated \textbf{DQN}, \textbf{Double DQN}, and
        \textbf{Dueling DQN} models on four Atari 2600 games from the Atari
        Learning Environment (ALE): breakout, road runner, boxing, and video
        pinball. The models were trained for 200 epochs (roughly 5M training
        steps) and evaluated for 100 episodes to get a mean score for each game.
        The mean scores were normalized the same way as in \cite{van2016deep} to
        show a comparison to human performance.
    \end{multicols}
}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{References}{name=references,column=0,above=bottom}{
    \smaller
  
    \bibliographystyle{ieee}
    \renewcommand{\section}[2]{\vskip 0.05em}
    \begin{thebibliography}{1}\itemsep=-0.01em
        \setlength{\baselineskip}{0.4em}

        \bibitem{mnih2015human}
        V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Rusu, A.~A., J.~Veness,
        M.G.~Bellemare, ... \& D.~Hassabis.
        \newblock Human-level Control through Deep Reinforcement Learning
        \newblock In {\em Nature, 518(7540), 529-533}, 2015.
        \bibitem{van2016deep}
        H.~van~Hasselt, A.~Guez, D.~Silver.
        \newblock Deep Reinforcement Learning with Double Q-learning
        \newblock In {\em Proceedings of the AAAI Conference on Artificial
        Intelligence}, 2016.

        \bibitem{wang2016dueling}
        Z.~Wang, T.~Schaul, M.~Hessel, H.~van~Hasselt, M.~Lanctot, and
        N.~Freitas.
        \newblock Dueling Network Architectures for Deep Reinforcement Learning
        \newblock In {\em International Conference on Machine Learning}, 2016.

  \end{thebibliography}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Mean Scores in Atari 2600 Games}{name=scores, column=2, span=2,
    below=results, above=bottom}{
    %\includegraphics[scale=0.47]{double_dqn-mean_scores}
    \includegraphics[width=\linewidth]{double_dqn-mean_scores}
    \includegraphics[width=\linewidth]{norm_scores}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Dueling Network Architecture}{name=dueling, column=1, span=1,
    above=bottom}{
    In some states it is important to know which action to take, but in many
    other states the choice of action is inconsequential to what happens next.
    For bootstrapping based algorithms, however, estimating the state values for
    every state is of great importance. 

    Motivated by this insight, the dueling network architecture uses \textbf{two
    streams of fully connected layers}, instead of one as in DQN. One layer
    provides an estimate for the value function, while the other estimates the
    advantage function. The two streams are combined to produce a single output
    $Q$ function using the following expression:
    \begin{align*}
    \begin{split}
        Q(s,a;\theta, & \alpha, \beta) = V(s; \theta, \beta) \\ 
            & + \left( A(s, a; \theta, \alpha) -
              \tfrac{1}{\abs{\mathcal{A}}} \sum_{a'}
              A(s, a'; \theta, \alpha \right)
    \end{split}
    \end{align*}
    This architecture leads to dramatic improvements over
    DQN and DDQN and produces state-of-the-art results.

    Since the inputs and outputs of the dueling network are the same as that of
    the original DQN, the training algorithm for DQN and Double DQN can also be
    used to train the dueling network.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Addressing Bias: Double DQN}{name=algorithm, column=1, above=dueling,
    below=dqn}{
    Despite being one of the most popular reinforcement algorithms,
    \textbf{Q-learning} performs poorly in some stochastic environments. The
    reason is that it produces overoptimistic estimates of the action values due
    to the max operator that is used for action selection and evaluation with
    the same network. \textbf{Double Q-learning} addresses this issue by
    decoupling action selection from action evaluation using two networks: an
    online network to select the greedy action, and a target network for
    evaluation.  The issue is also true for \textbf{Deep Q Networks}, which
    combine Q-learning with a flexible deep neural network to approximate the
    action-value function. 

    \textbf{Double DQN} introduces a new learning target where action selection is done
    according to the online network with parameters $\theta$, and action
    evaluation is done by the target network with parameters $\theta^-$.
    \begin{align*}
        \boxed{y_i^{\text{DDQN}} = r + \gamma \hat{Q} \left(s',
        \argmax_{a'} \hat{Q} \left(s', a'; \theta^-\right)\right)}
    \end{align*}
}
\end{poster}%
%
\end{document}
