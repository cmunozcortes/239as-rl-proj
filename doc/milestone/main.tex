\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% Silence warning about neurips package
\usepackage{silence}
\WarningFilter{latex}{You have requested package}

% ready for submission
%\usepackage{../neurips/neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{../neurips/neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{../neurips/neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{graphicx}

% Set bibliography style for natbib (called out by the nips style file)
\bibliographystyle{abbrvnat}

% argmax and argmin operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\title{ECE 239AS Reinforcement Learning\\
       Project Proposal}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
    Ryan Chau \\
    \texttt{chau\_ryan@yahoo.com}\\
    \And
    My-Quan Hong \\
    \texttt{myquan@yahoo.com} \\
    % examples of more authors
    \And
    Nathan Kang \\
    \texttt{nkang@gseis.ucla.edu} \\
    \And
    Christopher Munoz \\
    \texttt{cmunozcortes@ucla.edu} \\
}

\begin{document}

\maketitle

\section{Introduction}
% Motivation for DQN
Q-learning, one of the most popular reinforcement learning algorithms, is known
to overestimate action values because of the maximization step it includes. This
issue extends to Deep Q-learning networks (DQN), which combine Q-learning with a
flexible deep neural network to approximate the action-value function. Double
Q-learning, which was first proposed to alleviate the overestimation issue in
Q-learning, was subsequently shown to be effective in solving this problem in
DQN and lead to state-of-the-art results on the Atari environment.

% Methods
On this project we will attempt to replicate a subset of the results reported on
\cite{}. Namely, compare the action-value estimates on three Atari games, and
the normalized score on six games, tested for 100 episodes per game with human
starts. 

%------------------------------------------------------------------------------%
% TODO: Add motivation for clipped double Q-learning and methods
%------------------------------------------------------------------------------%



\section{Preliminary Results}

% Import bibliography from ref.bib
\bibliography{ref}

\end{document}