\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% Silence warning about neurips package
\usepackage{silence}
\WarningFilter{latex}{You have requested package}

% ready for submission
%\usepackage{../neurips/neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{../neurips/neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{../neurips/neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{graphicx}

% Set bibliography style for natbib (called out by the nips style file)
\bibliographystyle{abbrvnat}

% argmax and argmin operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\title{ECE 239AS Reinforcement Learning\\
       Project Milestone}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
    Ryan Chau \\
    \texttt{chau\_ryan@yahoo.com}\\
    \And
    My-Quan Hong \\
    \texttt{myquan@yahoo.com} \\
    % examples of more authors
    \And
    Nathan Kang \\
    \texttt{nkang@gseis.ucla.edu} \\
    \And
    Christopher Munoz \\
    \texttt{cmunozcortes@ucla.edu} \\
}

\begin{document}

\maketitle

\section{Introduction}
This objective of this project is to describe the methodology and
history of the algorithms described in \citet{van2016deep}, namely Q-learning,
Deep Q-learning networks (DQN), Double Q-learning, and Double DQN, as well as the 
Dueling DQN algorithm proposed in \citet{wang2015dueling}. In addition to summarizing
the history and methods of these algorithms, this project will also evaluate the
performance of DQN, Double Q-learning, Double DQN, and Dueling DQN
algorithms on four different Atari games.

% Motivation for DQN
Although Q-learning is one of the most popular reinforcement learning algorithms, it is known
to overestimate action values because of the included maximization step. This
issue extends to DQN, which combines Q-learning with a
flexible deep neural network to approximate the action-value function. Double
Q-learning, which was proposed to alleviate the overestimation issue in
Q-learning, was subsequently shown to be effective in solving the weaknesses of
DQN and producing state-of-the-art results in the Atari environment.

% Motivation for Clipped Double Q-learning
%Clipped Double Q-learning is another algorithm that addresses the inherent DQN 
%overestimation problem. The algorithm avoids overestimation by taking the minimum of the two next-state
%action values produced by the two Q networks. In the event that one Q estimate is 
%greater than the other, we reduce it to the minimum, thus preventing
%overestimation. Another benefit of this algorithm is that the minimum operator
%provides higher value to states with lower variance estimation error.  This
%means that the minimization will lead to a preference for states with
%low-variance value estimates, resulting in safer policy updates with stable
%learning targets.

% TODO: Provide motivation for Dueling DQN

% Methods
The Double DQN and Dueling DQN algorithms both address DQN's
overestimation bias. In this project we will compare the performance of DQN, 
Double Q-learning, Double DQN, and Dueling DQN by replicating 
a subset of the results reported in \cite{van2016deep}. Namely, this project 
will compare the action-value estimates of four Atari games, and the 
normalized mean score on four games, tested for 100 episodes per game
with human starts. 

\section{Preliminary Results}
The performance of DQN, Double DQN, and Dueling DQN was benchmarked on four
Atari games: Breakout, Road Runner, Bowling, and Video Pinball. The plots 
below show the mean score over 250 training epochs for DQN, Double DQN, 
and Dueling DQN. The performance of DQN and Double DQN was comparable on Road Runner and Boxing. However, Double DQN performed better than DQN on Breakout and Video Pinball, demonstrating the negative effects of DQN's overestimations.

% Import bibliography from ref.bib
\bibliography{ref}

\end{document}
