\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% Silence warning about neurips package
\usepackage{silence}
\WarningFilter{latex}{You have requested package}

% ready for submission
%\usepackage{../neurips/neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{../neurips/neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{../neurips/neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{graphicx}

% Set bibliography style for natbib (called out by the nips style file)
\bibliographystyle{abbrvnat}

% argmax and argmin operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\title{ECE 239AS Reinforcement Learning\\
       Project Proposal}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
    Ryan Chau \\
    \texttt{chau\_ryan@yahoo.com}\\
    \And
    My-Quan Hong \\
    \texttt{myquan@yahoo.com} \\
    % examples of more authors
    \And
    Nathan Kang \\
    \texttt{nkang@gseis.ucla.edu} \\
    \And
    Christopher Munoz \\
    \texttt{cmunozcortes@ucla.edu} \\
}

\begin{document}

\maketitle

\section{Introduction}
This objective of this project is to describe the methodology and
history of the algorithms described in \citet{van2016deep}, namely Q-learning,
DQN, Double Q-learning, and Double DQN, as well as the Clipped Double Q-learning
algorithm proposed in \citet{fujimoto2018addressing}. In addition to summarizing
the history and methods of these algorithms, this project will also evaluate the
performance of DQN, Double Q-learning, Double DQN, and Clipped Double Q-learning
algorithms on three different Atari games.

% Motivation for DQN
Q-learning, one of the most popular reinforcement learning algorithms, is known
to overestimate action values because of the included maximization step. This
issue extends to Deep Q-learning networks (DQN), which combine Q-learning with a
flexible deep neural network to approximate the action-value function. Double
Q-learning, which was proposed to alleviate the overestimation issue in
Q-learning, was subsequently shown to be effective in solving this problem in
DQN and lead to state-of-the-art results on the Atari environment.

% Methods
In this project we will attempt to replicate a subset of the results reported in
\cite{van2016deep}. Namely, this project will compare the action-value estimates of three Atari
games, and the normalized score on six games, tested for 100 episodes per game
with human starts. 

%------------------------------------------------------------------------------%
% TODO: Add motivation for clipped double Q-learning and methods
%------------------------------------------------------------------------------%



\section{Preliminary Results}

% Import bibliography from ref.bib
\bibliography{ref}

\end{document}
