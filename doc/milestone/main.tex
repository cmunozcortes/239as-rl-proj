\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% Silence warning about neurips package
\usepackage{silence}
\WarningFilter{latex}{You have requested package}

% ready for submission
%\usepackage{../neurips/neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{../neurips/neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{../neurips/neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%\usepackage{algorithm,algorithmicx}
\usepackage{algpseudocode,algorithm,setspace}

% Set bibliography style for natbib (called out by the nips style file)
\bibliographystyle{abbrvnat}

% argmax and argmin operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\multiline}[1]{%
  \begin{tabularx}{\dimexpr\linewidth-\ALG@thistlm}[t]{@{}X@{}}
    #1
  \end{tabularx}
}

\title{ECE 239AS Reinforcement Learning\\
       Project Milestone Report}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
    Ryan Chau \\
    \texttt{chau\_ryan@yahoo.com}\\
    \And
    My-Quan Hong \\
    \texttt{myquan@yahoo.com} \\
    % examples of more authors
    \And
    Nathan Kang \\
    \texttt{nkang@gseis.ucla.edu} \\
    \And
    Christopher Munoz \\
    \texttt{cmunozcortes@ucla.edu} \\
}

\begin{document}

\maketitle

\section{Introduction}
The objective of this project is to describe Double Deep Q Networks (DQN), an
algorithm first proposed in \citet{van2016deep} to reduce overoptimistic
action-value estimates in Deep Q Networks (\cite{mnih2015human}). A discussion
of another improvement on Deep Q Networks---Dueling DQN
(\cite{wang2016dueling})---will also be included for a more comprehensive
survey.  Finally, a comparison of the performance of all three algorithms on
four Atari games will be presented.

Although Q-learning is one of the most popular reinforcement learning
algorithms, it is known to overestimate action values because of the included
maximization step. This issue extends to Deep Q-learning networks (DQN), which
combines Q-learning with a flexible deep neural network to approximate the
action-value function. Double Q-learning, which was proposed to alleviate the
overestimation issue in Q-learning, was subsequently shown to be effective in
solving the weaknesses of DQN and producing state-of-the-art results in the
Atari environment.

\begin{algorithm}
%\setstretch{1.25}
\caption{Deep Q-learning with experience replay}\label{alg:dqn}
\begin{algorithmic}[1]
    \State Initialize replay memory $\mathcal{D}$ to capacity $N$
    \State Initialize action-value function $Q$ with random weights $\theta$
    \State Initialize target action-value function $\smash{\hat{Q}}$ with
    weights $\theta^- = \theta$
    \For{episode $m \gets 1,\dots,M$}
        \State Observe initial frame $x_1$ and preprocess frame to get state
        $s_1$
        \For{time step $t \gets 1,\dots,T$}
            \State With probability $\varepsilon$ select a random action $a_t$;
            otherwise select $a_t = \argmax_a Q(s_t, a; \theta)$
            \State Execute action $a_t$ in emulator and observe reward $r_t$ and
            image $x_{t+1}$
            \State Preprocess $s_t$, $x_{t+1}$ to get $s_{t+1}$
            \State Store transition $(s_t, a_t, r_t, s_{t+1})$ in replay buffer
            $\mathcal{D}$
            \State Sample uniformly a random minibatch of $N$ transitions $(s_j,
            a_j, r_j, s_{j+1})$ from $\mathcal{D}$
            \State Set $y_j = r_j$ if episode ends at step $j+1$; otherwise set
            $y_j = r_j + \gamma \max_{a'} \hat{Q}(s_{j+1}, a'; \theta^-)$
            \State Perform a stochastic gradient descent step on $J(\theta) =
            \tfrac{1}{N} \sum_{j=1}^{N} (y_j - Q(s_j, a_j; \theta))^2$ \newline
            \hspace*{4em} with respect to parameters $\theta$
            \State Every $c$ steps reset $\smash{\hat{Q} = Q}$
        \EndFor
    \EndFor
\end{algorithmic}
\end{algorithm}
%------------------------------------------------------------------------------%
% TODO: Nathan & Ryan to include discussion on Dueling DQN
%------------------------------------------------------------------------------%

\section{Preliminary Results}
To conduct an evaluation of the performance of the algorhtms we used an
implementation based on the Tensorpack framework (\cite{wu2016tensorpack}).
Tensorpack is an interface built on TensorFlow v1 that is focused on training
speed. Its low overhead and support for training with a GPU provided us with a
way to train the models from scratch in Google Colab Pro instances, within a
reasonable amount of time (roughly 24 hours for each Atari game).

Each model was trained for 6.225 million training steps (250 epochs) on the
following games from the Atari Learning Environment (ALE)
(\cite{bellemare2013arcade}): \texttt{Breakout-v0}, \texttt{RoadRunner-v0},
\texttt{Boxing-v0}, and \texttt{VideoPinball-v0}.

Figure \ref{fig:training} shows the scores achieved by each trained model for
each of the four Atari games.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{double_dqn-mean_scores.png}
    \caption{Mean score over training steps for selected games from the Atari
    Learning Environment (ALE).}
    \label{fig:training}
\end{figure}

The mean scores achieved by DQN and Double DQN were comparable on Road Runner
and Boxing. However, Double DQN performed better than DQN on Breakout and Video
Pinball, demonstrating the negative effects of DQN's overestimations.

% Import bibliography from ref.bib
\bibliography{ref}

\end{document}
