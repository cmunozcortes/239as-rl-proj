\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% Silence warning about neurips package
\usepackage{silence}
\WarningFilter{latex}{You have requested package}

% ready for submission
%\usepackage{../neurips/neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{../neurips/neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{../neurips/neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{graphicx}

% Set bibliography style for natbib (called out by the nips style file)
\bibliographystyle{abbrvnat}

% argmax and argmin operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\title{ECE 239AS Reinforcement Learning\\
       Project Proposal}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
    Ryan Chau \\
    \texttt{chau\_ryan@yahoo.com}\\
    \And
    My-Quan Hong \\
    \texttt{myquan@yahoo.com} \\
    % examples of more authors
    \And
    Nathan Kang \\
    \texttt{nkang@gseis.ucla.edu} \\
    \And
    Christopher Munoz \\
    \texttt{cmunozcortes@ucla.edu} \\
}

\begin{document}

\maketitle

\section{Introduction}
This objective of this project is to describe the methodology and
history of the algorithms described in \citet{van2016deep}, namely Q-learning,
DQN, Double Q-learning, and Double DQN, as well as the Clipped Double Q-learning
algorithm proposed in \citet{fujimoto2018addressing}. In addition to summarizing
the history and methods of these algorithms, this project will also evaluate the
performance of DQN, Double Q-learning, Double DQN, and Clipped Double Q-learning
algorithms on three different Atari games.

% Motivation for DQN
Although Q-learning is one of the most popular reinforcement learning algorithms, it is known
to overestimate action values because of the included maximization step. This
issue extends to Deep Q-learning networks (DQN), which combines Q-learning with a
flexible deep neural network to approximate the action-value function. Double
Q-learning, which was proposed to alleviate the overestimation issue in
Q-learning, was subsequently shown to be effective in solving the weaknesses of
DQN and producing state-of-the-art results in the Atari environment.

% Motivation for Clipped Double Q-learning
Clipped Double Q-learning is another algorithm that addresses the inherent DQN 
overestimation problem. The algorithm avoids overestimation by taking the minimum of the two next-state
action values produced by the two Q networks. In the event that one Q estimate is 
greater than the other, we reduce it to the minimum, thus preventing
overestimation. Another benefit of this algorithm is that the minimum operator
provides higher value to states with lower variance estimation error.  This
means that the minimization will lead to a preference for states with
low-variance value estimates, resulting in safer policy updates with stable
learning targets.

% Methods
The Double DQN and Clipped Double Q-learning algorithms both address DQN's
overestimation bias. In this project we will compare the performance of DQN, 
Double Q-learning, Double DQN, and Clipped Double Q-learning by replicating 
a subset of the results reported in \cite{van2016deep}. Namely, this project 
will compare the action-value estimates of three Atari games, and the 
normalized score on six games, tested for 100 episodes per game
with human starts. 




\section{Preliminary Results}

% Import bibliography from ref.bib
\bibliography{ref}

\end{document}
